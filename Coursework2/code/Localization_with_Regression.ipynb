{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Localization_with_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5YBsdeoakjv",
        "colab_type": "text"
      },
      "source": [
        "# Object Detection with Bounding Box Regression\n",
        "\n",
        "This notebook will demonstrate some of the basic steps in object detection. Bounding box regression, as the name suggests, makes a regressor model which directly outputs the bounding box coordinates given an image.\n",
        "\n",
        "This concept has been internal to object detectors like [YOLO](https://arxiv.org/abs/1506.02640).\n",
        "\n",
        "Note: If you are running this notebook on Colab itself, I insist you turn on the GPU runtime. Go to **Tools ~> Runtime ~> Change runtime type ~> Hardware Accelerator ~> Select \"GPU\"**\n",
        "\n",
        "Let's Start!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWtbHg3UddHf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1) Importing the packages\n",
        "\n",
        "We import TensorFlow and Keras along with our beloved NumPy. We print the TensorFlow version we're using.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTg_2ZBqFM2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyVzlExRduu-",
        "colab_type": "text"
      },
      "source": [
        "## 2) Downloading the data from GitHub\n",
        "\n",
        "We need to download our dataset from the [Dataset_Archives](https://github.com/shubham0204/Dataset_Archives) repo where I have hosted a ZIP file of the dataset. The original dataset hails from [Kaggle.com](https://www.kaggle.com) as [Image Localization Dataset](https://www.kaggle.com/mbkinaci/image-localization-dataset) by [Muhammed Buyukkinaci](https://www.kaggle.com/mbkinaci).\n",
        "\n",
        "Also, we install the [xmltodict](https://pypi.org/project/xmltodict/) package from PyPI which will be useful to parse the XML files containing the bounding box annotations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dIf16k6FT68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install xmltodict\n",
        "!wget https://github.com/shubham0204/Dataset_Archives/blob/master/image-localization-dataset.zip?raw=true -O data.zip\n",
        "!unzip data.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DERx3xbf8zH",
        "colab_type": "text"
      },
      "source": [
        "## 4) Parsing the images from the dataset\n",
        "\n",
        "We prepare a list of all the files under `training_images` folder which have a `*.jpg` extension ( image files ) using `glob`. We resize them to the `input_dim` and normalize them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP8pq3CFFW-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from PIL import Image , ImageDraw\n",
        "import os\n",
        "import glob\n",
        "\n",
        "input_dim = 228\n",
        "\n",
        "images = []\n",
        "image_paths = glob.glob( 'training_images/*.jpg' )\n",
        "for imagefile in image_paths:\n",
        "    image = Image.open( imagefile ).resize( ( input_dim , input_dim ))\n",
        "    image = np.asarray( image ) / 255.0\n",
        "    images.append( image )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgu_4WTPggsO",
        "colab_type": "text"
      },
      "source": [
        "## 5) Parsing the bounding box annotations\n",
        "\n",
        "We parse all XML files under the `training_images` folder. These annotations are in the popular `PASCAL-VOC` format. We extract the four bounding box coordinates and normalize them using `input_dim`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sUew-O2FaSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import xmltodict\n",
        "import os\n",
        "\n",
        "bboxes = []\n",
        "annotations_paths = glob.glob( 'training_images/*.xml' )\n",
        "for xmlfile in annotations_paths:\n",
        "    x = xmltodict.parse( open( xmlfile , 'rb' ) )\n",
        "    bndbox = x[ 'annotation' ][ 'object' ][ 'bndbox' ]\n",
        "    bndbox = np.array([ int(bndbox[ 'xmin' ]) , int(bndbox[ 'ymin' ]) , int(bndbox[ 'xmax' ]) , int(bndbox[ 'ymax' ]) ])\n",
        "    bboxes.append( bndbox / input_dim )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHvyZGlYhMYB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 6) Preparing the final datasets\n",
        "\n",
        "Our input will be an array of all the images we parsed earlier. The target output will be the bounding box coordinates.\n",
        "\n",
        "Using scikit-learn's `train_test_split` method, we split our dataset in training and validation datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYl6unq-Fc3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Y = np.array( bboxes )\n",
        "X = np.array( images )\n",
        "\n",
        "Y = np.reshape( Y , ( -1 , 1 , 1 , 4 ) )\n",
        "\n",
        "print( X.shape ) \n",
        "print( Y.shape )\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split( X, Y, test_size=0.1 )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp5mYj7Ylauo",
        "colab_type": "text"
      },
      "source": [
        "## 7) Defining the loss function and evaluation metrics\n",
        "\n",
        "First, we define a method which calculates the intersection over union ( IOU ) of two bounding boxes. In the `custom_loss`, we first calculate Mean Squared Error of the target and predicted bounding boxes. Next, we calculate the IOU loss which is $1 - iou$.\n",
        "\n",
        "Hence, our final loss function is like,\n",
        "\n",
        "$\\Large L( y , y' ) = MSE( y, y') + ( 1 - IOU( y , y' ))$\n",
        "\n",
        "Also, we use IOU as a validation metric to evaluate the model's performance on the testing dataset.\n",
        "\n",
        "**Note: The loss function is a custom implementation. IOU is mostly used as a metric and not in the loss function.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPuyqOpQhPiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "input_shape = ( input_dim , input_dim , 3 )\n",
        "\n",
        "def calculate_iou( target_boxes , pred_boxes ):\n",
        "    xA = K.maximum( target_boxes[ ... , 0], pred_boxes[ ... , 0] )\n",
        "    yA = K.maximum( target_boxes[ ... , 1], pred_boxes[ ... , 1] )\n",
        "    xB = K.minimum( target_boxes[ ... , 2], pred_boxes[ ... , 2] )\n",
        "    yB = K.minimum( target_boxes[ ... , 3], pred_boxes[ ... , 3] )\n",
        "    interArea = K.maximum( 0.0 , xB - xA ) * K.maximum( 0.0 , yB - yA )\n",
        "    boxAArea = (target_boxes[ ... , 2] - target_boxes[ ... , 0]) * (target_boxes[ ... , 3] - target_boxes[ ... , 1])\n",
        "    boxBArea = (pred_boxes[ ... , 2] - pred_boxes[ ... , 0]) * (pred_boxes[ ... , 3] - pred_boxes[ ... , 1])\n",
        "    iou = interArea / ( boxAArea + boxBArea - interArea )\n",
        "    return iou\n",
        "\n",
        "def custom_loss( y_true , y_pred ):\n",
        "    mse = tf.losses.mean_squared_error( y_true , y_pred ) \n",
        "    iou = calculate_iou( y_true , y_pred ) \n",
        "    return mse + ( 1 - iou )\n",
        "\n",
        "def iou_metric( y_true , y_pred ):\n",
        "    return calculate_iou( y_true , y_pred ) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hidqxtGNn0-g",
        "colab_type": "text"
      },
      "source": [
        "## 8) Making the CNN model\n",
        "\n",
        "We'll use a CNN model which takes in input an image of shape `( input_dim , input_dim , 3 )` and outputs a vector of shape `( 1 , 1 , 4 )`.\n",
        "\n",
        "*   The `Conv2D` layers will extract features from the image.\n",
        "*   We will not use `Dense` layers as they degrade the performance of our model.\n",
        "\n",
        "We use a small learning rate of 0.0001 so that the learning doesn't stop. Also, we pass the `iou_metric` in the `model.compile()` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SITou1wjFwPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model_layers = [       \n",
        "    keras.layers.Conv2D( 256 , input_shape=( input_dim , input_dim , 3 ) , kernel_size=( 3 , 3 ) , strides=2 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=2 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    \n",
        "    keras.layers.Conv2D( 16 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 16 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
        "\n",
        "    keras.layers.Conv2D( 4 , kernel_size=( 2 , 2 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 4 , kernel_size=( 2 , 2 ) , strides=1 , activation='relu' ),\n",
        "    keras.layers.Conv2D( 4 , kernel_size=( 2 , 2 ) , strides=1 , activation='sigmoid' ),\n",
        "]\n",
        "\n",
        "model = keras.Sequential( model_layers )\n",
        "model.compile(\n",
        "\toptimizer=keras.optimizers.Adam( lr=0.0001 ),\n",
        "\tloss=custom_loss,\n",
        "    metrics=[ iou_metric ]\n",
        ")\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PwqHt-YpP8W",
        "colab_type": "text"
      },
      "source": [
        "## 9) Training the model\n",
        "\n",
        "We finally train the model with the validation dataset and save it to the disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqYpnDhlIiJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.fit( \n",
        "    x_train ,\n",
        "    y_train , \n",
        "    validation_data=( x_test , y_test ),\n",
        "    epochs=50 ,\n",
        "    batch_size=5 \n",
        ")\n",
        "\n",
        "model.save( 'model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-JywIONpb6W",
        "colab_type": "text"
      },
      "source": [
        "## 10) Testing the model on test images\n",
        "\n",
        "The following code will predict bounding boxes for some unseen images, draw them on the image using `ImageDraw` and then save them to a folder ( created with `mkdir` command )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcn3zdf-I5yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!rm -r inference_images\n",
        "!mkdir -v inference_images\n",
        "\n",
        "boxes = model.predict( x_test )\n",
        "for i in range( boxes.shape[0] ):\n",
        "    b = boxes[ i , 0 , 0 , 0 : 4 ] * input_dim\n",
        "    img = x_test[i] * 255\n",
        "    source_img = Image.fromarray( img.astype( np.uint8 ) , 'RGB' )\n",
        "    draw = ImageDraw.Draw( source_img )\n",
        "    draw.rectangle( b , outline=\"black\" )\n",
        "    source_img.save( 'inference_images/image_{}.png'.format( i + 1 ) , 'png' )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMysv18NqMMI",
        "colab_type": "text"
      },
      "source": [
        "## 11) Print the average IOU\n",
        "\n",
        "We will calculate the average IOU score over the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofgrJJxEOpOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def calculate_avg_iou( target_boxes , pred_boxes ):\n",
        "    xA = np.maximum( target_boxes[ ... , 0], pred_boxes[ ... , 0] )\n",
        "    yA = np.maximum( target_boxes[ ... , 1], pred_boxes[ ... , 1] )\n",
        "    xB = np.minimum( target_boxes[ ... , 2], pred_boxes[ ... , 2] )\n",
        "    yB = np.minimum( target_boxes[ ... , 3], pred_boxes[ ... , 3] )\n",
        "    interArea = np.maximum(0.0, xB - xA ) * np.maximum(0.0, yB - yA )\n",
        "    boxAArea = (target_boxes[ ... , 2] - target_boxes[ ... , 0]) * (target_boxes[ ... , 3] - target_boxes[ ... , 1])\n",
        "    boxBArea = (pred_boxes[ ... , 2] - pred_boxes[ ... , 0]) * (pred_boxes[ ... , 3] - pred_boxes[ ... , 1])\n",
        "    iou = interArea / ( boxAArea + boxBArea - interArea )\n",
        "    return iou\n",
        "\n",
        "target_boxes = y_test * input_dim\n",
        "pred = model.predict( x_test )\n",
        "pred_boxes = pred[ ... , 0 : 4 ] * input_dim\n",
        "\n",
        "iou_scores = calculate_avg_iou( target_boxes , pred_boxes )\n",
        "print( 'Mean IOU score {}'.format( iou_scores.mean() ) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdJoOVBoY5AI",
        "colab_type": "text"
      },
      "source": [
        "## ( Optional ) Convert the Keras model to TensorFlow Lite model\n",
        "\n",
        "We can convert the Keras models ( .h5 ) to a TensorFlow Lite model ( .tflite ) for using it with Android or iOS devices.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfKGZOTcYZbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model_file( 'model.h5' , custom_objects={ 'custom_loss' : custom_loss , 'iou_metric' : iou_metric } ) \n",
        "converter.post_training_quantize = True\n",
        "buffer = converter.convert()\n",
        "open( 'model.tflite' , 'wb' ).write( buffer )\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}