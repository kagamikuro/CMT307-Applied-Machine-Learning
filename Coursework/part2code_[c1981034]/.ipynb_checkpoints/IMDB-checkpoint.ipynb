{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tianbai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "import requests\n",
    "import operator\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load IMDB data\n",
    "path_train_pos = \"../datasets_coursework1/IMDb/train/imdb_train_pos.txt\"\n",
    "path_train_neg = \"../datasets_coursework1/IMDb/train/imdb_train_neg.txt\"\n",
    "path_test_pos = \"../datasets_coursework1/IMDb/test/imdb_test_pos.txt\"\n",
    "path_test_neg = \"../datasets_coursework1/IMDb/test/imdb_test_neg.txt\"\n",
    "path_dev_pos = \"../datasets_coursework1/IMDb/dev/imdb_dev_pos.txt\"\n",
    "path_dev_neg = \"../datasets_coursework1/IMDb/dev/imdb_dev_neg.txt\"\n",
    "\n",
    "dataset_train_pos =open(path_train_pos,'rb').readlines()\n",
    "dataset_train_neg =open(path_train_neg,'rb').readlines()\n",
    "dataset_test_pos =open(path_test_pos,'rb').readlines()\n",
    "dataset_test_neg =open(path_test_neg,'rb').readlines()\n",
    "dataset_dev_pos =open(path_dev_pos,'rb').readlines()\n",
    "dataset_dev_neg =open(path_dev_neg,'rb').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training data: 7483\n",
      "\n",
      "Number of negative training data: 7517\n",
      "\n",
      "Number of positive testing data: 2499\n",
      "\n",
      "Number of negative testing data: 2501\n",
      "\n",
      "Number of positive develop data: 2518\n",
      "\n",
      "Number of negative develop data: 2482\n",
      "\n",
      "b'For fans of Chris Farley, this is probably his best film. David Spade plays the perfect cynical, sarcastic yin to Farley\\'s \"Baby Huey\" yang. Farley achieves strokes of comic genius in his monologues, like the \"Let\\'s say you\\'re driving along the road with your family...\" bit, the \"Jo-Jo the Idiot Circus Boy with a pretty new pet, (his possible sale)\" speech, or the \"Glue-sniffing Guarantee fairy\" brake pad sale. The sappy moments in the film contrast sharply with Farley and Spade\\'s shenanigans. Even after many viewings, it\\'s still fun to see Farley pour everything he had into the role. \"Richard, what\\'s HAPPENING to me?!?!\"\\n'\n",
      "b\"Fantastic, Madonna at her finest, the film is funny and her acting is brilliant. It may have been made in the 80's but it has all the qualities of a modern Hollywood Block-buster. I love this film and i think its totally unique and will cheer up any droopy person within a matter of minutes. Fantastic.\\n\"\n",
      "b\"From a perspective that it is possible to make movies that are not offensive to people with strong moral values, this one is definitely worthwhile. This is the second Bruce Willis film in a row that manages to tell its story with no nudity, off-color humor, profanity, or gratuitous violence. (I refer of course to The Sixth Sense.) Both movies are engaging on more than one level. This one is appropriate for children as well, although as others have pointed out, it isn't a flick FOR kids. <br /><br />I was bothered that the time travel device that drives this plot is never explained, except that we know Russell himself initiates it as a 70 year old. Also, why does his dying mother have to come to school to get him when he wins the fight; why, if as his older self says, he has to fight that kid again and again for the next few years does his mother not have to come and get him every time, and why he doesn't learn to kick butt in the process. I also found the score rather annoying and not always appropriate to the action on stage. <br /><br />Good use of the red plane as metaphor, however.\\n\"\n",
      "b'What is often neglected about Harold Lloyd is that he was an actor. Unlike Chaplin and Keaton, Lloyd didn\\'t have the Vaudeville/Music Hall background and he wasn\\'t a natural comedian. He came to Hollywood to act; and he discovered he had a knack for acting funny -- first in shorts, then in features. He made a name for himself as \"Lonesome Luke\", a Chaplin knock-off; with the \"glasses character\" that made him the all-American boy rather than a grotesque, Lloyd found his stride and his movies became some of the best produced during the silent era.<br /><br />He developed a reputation as a \"daredevil\" in some shorts, and retained this in some of his best movies (\"Safety Last\", \"For Heaven\\'s Sake\", \"Girl Shy\"). He was more popular than either Chaplin or Keaton during the twenties and he became very rich before the advent of sound.<br /><br />The first sound movies were often disasters. To get the most out of their \"sound\", too much dialog was used in many movies.<br /><br />Lloyd\\'s acting skills were, after two decades, geared for silents. He didn\\'t have a bad voice; its high pitch suited his \"glasses\" character. And his sound films weren\\'t the unqualified disasters of legend. Yet silent movies had been raised to a high art (especially Lloyd\\'s, which did not stint on budget and were extremely well-crafted); with the introduction of talkies movies had to learn to walk again and they made some missteps.<br /><br />Though he tried to move with the times and embraced sound, Lloyd\\'s best bits from his early (overly talky) talkies were still visual -- such as the scene in \"Movie Crazy\" where he appears to be riding in a swank car, but actually \"hitched a ride\" on his bicycle.<br /><br />Trying to recapture the daredevil antics that made him famous, as he did in \"Feet First\", was misstep. (In \"Safety Last\", his best movie and the one that, deservedly or not, shoved Lloyd in the box as a \"daredevil comic\", he played a determined young man, climbing to the top. \"Safety Last\" had a natural structure that ascended to his character\\'s scaling the side of the building. He was obviously afraid, but his fear added to the humor. In \"Feet First\", he arrived in a precarious building-scaling position by accident; his frantic cries for help detracted from the humor. His character was pathetic and cringing, aspiration to save his neck -- possibly an accurate statement of the 1930s, but not amusing).<br /><br />Harold Lloyd was not mired in the past, like some wacky Norma Desmond. He embraced sound and tried to take his movies in different directions, growing and changing with the industry. When \"Feet First\" failed he left the daredevil business and made a satire on the talking movie industry, \"Movie Crazy\". Just as he had to flounder through many movies as \"Lonesome Luke\" before carving his place in movie history with the glasses character, he had tried several directions in sound movies before hitting his stride in sound, which he did with \"The Catspaw\".<br /><br />In \"The Catspaw\" he plays a missionary\\'s son reared in China who unwittingly gets elected mayor as a front for corrupt political interests. When he finds out the truth, he sets himself the task of cleaning up the town. Only in his early forties, Lloyd could still act the brash young man.<br /><br />Yet \"The Catspaw\" was another box-office failure, and Lloyd made only three more movies, including \"The Milky Way\". Of his chief competitors, Chaplin still had silent movies in him and Keaton was hopelessly mismanaged. \"The Catspaw\" and \"The Milky Way\" suggest Lloyd might have mastered sound comedy if he had been a little younger, or if audiences had given him the benefit of the doubt after his early sound fiascoes.<br /><br />Though the movie has been unfairly maligned about the way Lloyd\\'s character cleaned up the town, it suits him. From his days in \"shorts\" Lloyd wanted to scare his audience, and the climax of \"The Catspaw\" achieved it yet again, in a surprising way; until the trick is revealed it appears gruesome, and then come the laughs.<br /><br />Viewed as a product of its time, \"The Catspaw\" is charming and funny. A very well-written sound comedy, well-acted by Lloyd. Directed by Sam Taylor, its curious blend of drama and sly humor make it look almost like a Frank Capra or Preston Sturges comedy.\\n'\n",
      "b'You\\'ll either love or hate movies such as this thriller set inside a lonesome asylum in a far off lonesome land. It\\'s not so much of a horror show, but a concoction of frightening imageries and wackozoid mental patients. \"Scream\" is the best term to use in what was obviously a popular drive-in classic noted for some strange and wicked behaviors. Notice the \"judge\", who\\'s about to put on the ax from behind the doctor! Brr-r-r-r!!! Not much else can be described here other than some bloody tasty goodness, but when you get a chance, remember the familiar old saying by the hag lady: \"Get out! Get out! And never ever come back!\". Don\\'t you wish you haven\\'t looked in the basement?\\n'\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of positive training data: \"+str(len(dataset_train_pos))+\"\\n\")\n",
    "print (\"Number of negative training data: \"+str(len(dataset_train_neg))+\"\\n\")\n",
    "print (\"Number of positive testing data: \"+str(len(dataset_test_pos))+\"\\n\")\n",
    "print (\"Number of negative testing data: \"+str(len(dataset_test_neg))+\"\\n\")\n",
    "print (\"Number of positive develop data: \"+str(len(dataset_dev_pos))+\"\\n\")\n",
    "print (\"Number of negative develop data: \"+str(len(dataset_dev_neg))+\"\\n\")\n",
    "\n",
    "for i in dataset_train_pos[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 15000\n",
      "Size of test set: 5000\n"
     ]
    }
   ],
   "source": [
    "train_set=[]\n",
    "test_set=[]\n",
    "\n",
    "# combine positive and negitive review together as train set and test set\n",
    "# and decode byte data to string\n",
    "for pos_review in dataset_train_pos:\n",
    "  pos_review_str = pos_review.decode();\n",
    "  train_set.append((pos_review_str,1))\n",
    "for neg_review in dataset_train_neg:\n",
    "  neg_review_str = neg_review.decode(); \n",
    "  train_set.append((neg_review_str,0))\n",
    "random.shuffle(train_set)\n",
    "\n",
    "for pos_review in dataset_test_pos:\n",
    "  pos_review_str = pos_review.decode();\n",
    "  test_set.append((pos_review_str,1))\n",
    "for neg_review in dataset_test_neg:\n",
    "  neg_review_str = neg_review.decode();\n",
    "  test_set.append((neg_review_str,0))\n",
    "random.shuffle(test_set)\n",
    "\n",
    "print (\"Size of training set: \"+str(len(train_set)))\n",
    "print (\"Size of test set: \"+str(len(test_set)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def get_list_tokens(string):\n",
    "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "  list_tokens=[]\n",
    "  for sentence in sentence_split:\n",
    "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "    for token in list_tokens_sentence:\n",
    "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "  return list_tokens\n",
    "\n",
    "\n",
    "# First, we get the stopwords list from nltk\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "# We can add more words to the stopword list, like punctuation marks\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "stopwords.add(\"#\")\n",
    "stopwords.add(\"@\")\n",
    "stopwords.add(\":\")\n",
    "stopwords.add(\"1\")\n",
    "stopwords.add(\"0\")\n",
    "stopwords.add(\"/\")\n",
    "stopwords.add(\">\")\n",
    "stopwords.add(\"<\")\n",
    "stopwords.add(\"(\")\n",
    "stopwords.add(\")\")\n",
    "\n",
    "\n",
    "# Now we create a frequency dictionary with all words in the dataset\n",
    "# This can take a few minutes depending on your computer, since we are processing more than ten thousand sentences\n",
    "\n",
    "# Function taken from Session 1\n",
    "def get_list_tokens(string): # Function to retrieve the list of tokens from a string\n",
    "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "  list_tokens=[]\n",
    "  for sentence in sentence_split:\n",
    "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "    for token in list_tokens_sentence:\n",
    "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "  return list_tokens\n",
    "\n",
    "\n",
    "def get_list_ADJ_VERB_tokens(string): # Function to retrieve the list of tokens from a string\n",
    "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "  list_tokens=[]\n",
    "  for sentence in sentence_split:\n",
    "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "    list_tokens_sentence_tag =nltk.pos_tag(list_tokens_sentence);\n",
    "    #print(list_tokens_sentence_tag)\n",
    "    for token,tag in list_tokens_sentence_tag:\n",
    "      if tag =='ADJ' or tag =='VB' or tag =='VBP' or tag =='VBG' :\n",
    "          list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "  return list_tokens\n",
    "\n",
    "# Function taken from Session 2\n",
    "def get_vector_text(list_vocab,string):\n",
    "  vector_text=np.zeros(len(list_vocab))\n",
    "  list_tokens_string=get_list_tokens(string)\n",
    "  for i, word in enumerate(list_vocab):\n",
    "    if word in list_tokens_string:\n",
    "      vector_text[i]=list_tokens_string.count(word)\n",
    "  return vector_text\n",
    "\n",
    "\n",
    "# Functions slightly modified from Session 2\n",
    "\n",
    "def get_vocabulary(training_set, num_features, type_features): # Function to retrieve vocabulary\n",
    "  dict_word_frequency={}\n",
    "  for instance in training_set:\n",
    "    sentence_tokens = []\n",
    "    if type_features == \"totalWordsFrequency\":\n",
    "        sentence_tokens=get_list_tokens(instance[0])#for feature type 1\n",
    "    if type_features == \"Adj_VerbWordsFrequency\":\n",
    "        sentence_tokens=get_list_ADJ_VERB_tokens(instance[0])#for feature type 2\n",
    "    else:\n",
    "        sentence_tokens=get_list_tokens(instance[0])#default\n",
    "    for word in sentence_tokens:\n",
    "      if word in stopwords: continue\n",
    "      if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "      else: dict_word_frequency[word]+=1\n",
    "  sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "  vocabulary=[]\n",
    "  for word,frequency in sorted_list:\n",
    "    vocabulary.append(word)\n",
    "  i=0  \n",
    "  for word,frequency in sorted_list[:10]:\n",
    "      i+=1\n",
    "      print (str(i)+\". \"+word+\" - \"+str(frequency))\n",
    "  return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "def train_svm_classifier(training_set, vocabulary): # Function for training our svm classifier\n",
    "  X_train=[]\n",
    "  Y_train=[]\n",
    "  for instance in training_set:\n",
    "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "    X_train.append(vector_instance)\n",
    "    Y_train.append(instance[1])\n",
    "    \n",
    "  X_train = np.asarray(X_train)\n",
    "  Y_train = np.asarray(Y_train) \n",
    "    \n",
    "    \n",
    "  fs_sentanalysis=SelectKBest(chi2, k=1000).fit(X_train, Y_train)\n",
    "  X_train_new = fs_sentanalysis.transform(X_train)   \n",
    "\n",
    "  #X_train_new = SelectKBest(chi2, k=1000).fit_transform(X_train, Y_train)\n",
    "  print (\"Size original training matrix: \"+str(X_train.shape))\n",
    "  print (\"Size new training matrix: \"+str(X_train_new.shape))  \n",
    "    \n",
    "  # Finally, we train the SVM classifier \n",
    "  svm_clf=svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "  #svm_clf.fit(np.asarray(X_train),np.asarray(Y_train))\n",
    "  svm_clf.fit(X_train_new,Y_train)\n",
    "  return svm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. br - 59548\n",
      "2. 's - 36104\n",
      "3. movie - 29648\n",
      "4. wa - 29577\n",
      "5. film - 26929\n",
      "6. '' - 19857\n",
      "7. n't - 19640\n",
      "8. one - 15987\n",
      "9. ! - 14847\n",
      "10. like - 11876\n",
      "total num of features:1000\n"
     ]
    }
   ],
   "source": [
    "# add features of the total frequency of words to vocabulary\n",
    "vocabulary=get_vocabulary(train_set, 1000,\"totalWordsFrequency\")  # We use the get_vocabulary function to retrieve the vocabulary\n",
    "print(\"total num of features:\"+str(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. see - 6605\n",
      "2. get - 5218\n",
      "3. make - 4502\n",
      "4. think - 4141\n",
      "5. know - 3735\n",
      "6. watch - 3619\n",
      "7. say - 3153\n",
      "8. 've - 2974\n",
      "9. 'm - 2795\n",
      "10. watching - 2602\n",
      "total num of features:2000\n"
     ]
    }
   ],
   "source": [
    "# add features of the frequency of ADJ and VERB words to vocabulary,and this can take a while...\n",
    "vocabulary.extend(get_vocabulary(train_set, 1000,\"Adj_VerbWordsFrequency\"))  \n",
    "print(\"total num of features:\"+str(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature: 1000\n",
      "\n",
      "total num of features:3000\n"
     ]
    }
   ],
   "source": [
    "#Add features of key words to vocabulary, by using CountVectorizer \n",
    "\n",
    "X_train=[]\n",
    "\n",
    "for instance in train_set:\n",
    "    X_train.append(instance[0])\n",
    "  \n",
    "vectorizer = CountVectorizer(max_features = 1000)\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "word = vectorizer.get_feature_names()\n",
    "\n",
    "print (\"Number of feature: \"+str(len(word))+\"\\n\")\n",
    "#print( '\\nvocabulary dic :\\n\\n',vectorizer.vocabulary_)\n",
    "\n",
    "vocabulary.extend(word)  \n",
    "print(\"total num of features:\"+str(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size original training matrix: (15000, 3000)\n",
      "Size new training matrix: (15000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# reduce dimension from 3000 to 1000 and train SVM classifier. This can take for a long time...\n",
    "svm_clf=train_svm_classifier(train_set, vocabulary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 3000 should be equal to 1000, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-2c4b608271e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_vector_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" This was a complete waste of celluloid. The preview was promising but after watching the mov\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \"\"\"\n\u001b[1;32m--> 567\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \"\"\"\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    476\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0;32m    477\u001b[0m                              \u001b[1;34m\"the number of features at training time\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m                              (n_features, self.shape_fit_[1]))\n\u001b[0m\u001b[0;32m    479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X.shape[1] = 3000 should be equal to 1000, the number of features at training time"
     ]
    }
   ],
   "source": [
    "print (svm_clf.predict(fs_sentanalysis.transform([get_vector_text(vocabulary,\" This was a complete waste of celluloid. The preview was promising but after watching the mov\")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 3000 should be equal to 1000, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-7e9b6ffee808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mY_test_gold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mY_text_predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_gold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_text_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \"\"\"\n\u001b[1;32m--> 567\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \"\"\"\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    476\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0;32m    477\u001b[0m                              \u001b[1;34m\"the number of features at training time\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m                              (n_features, self.shape_fit_[1]))\n\u001b[0m\u001b[0;32m    479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X.shape[1] = 3000 should be equal to 1000, the number of features at training time"
     ]
    }
   ],
   "source": [
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in test_set:\n",
    "  vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "  X_test.append(vector_instance)\n",
    "  Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "Y_text_predictions=svm_clf.predict(fs_sentanalysis.transform(X_test))\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      2501\n",
      "           1       0.85      0.86      0.85      2518\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      5019\n",
      "   macro avg       0.85      0.85      0.85      5019\n",
      "weighted avg       0.85      0.85      0.85      5019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_set=[]\n",
    "for pos_review in dataset_dev_pos:\n",
    "  pos_review_str = pos_review.decode();\n",
    "  dev_set.append((pos_review_str,1))\n",
    "for neg_review in dataset_test_neg:\n",
    "  neg_review_str = neg_review.decode();\n",
    "  dev_set.append((neg_review_str,0))\n",
    "random.shuffle(dev_set)\n",
    "\n",
    "\n",
    "X_dev=[]\n",
    "Y_dev=[]\n",
    "for instance in dev_set:\n",
    "  vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "  X_dev.append(vector_instance)\n",
    "  Y_dev.append(instance[1])\n",
    "X_test=np.asarray(X_dev)\n",
    "Y_test_gold=np.asarray(Y_dev)\n",
    "Y_text_predictions=svm_clf.predict(X_test)\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
