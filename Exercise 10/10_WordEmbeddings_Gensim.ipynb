{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_WordEmbeddings_Gensim.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG7F15TeVuxb",
        "colab_type": "text"
      },
      "source": [
        "This Jupyter notebook shows how to train and use word embeddings with the [Gensim](https://radimrehurek.com/gensim/) library. Some snippets have been adapted from the exercises of the course of Dr Luis Espinosa and from the [Gensim's Word2Vec tutorial](https://rare-technologies.com/word2vec-tutorial/).\n",
        "\n",
        "Word embeddings are vector representations of words which are generally low-dimensional (often less than 1000 dimensions).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXTVxIXysM9O",
        "colab_type": "text"
      },
      "source": [
        "## TRAINING WORD EMBEDDINGS (Word2Vec)\n",
        "\n",
        "---\n",
        "\n",
        "As usual, we first import the libraries that we are going to use, including now Gensim.\n",
        "\n",
        "**Note:** All these libraries need to be downloaded beforehand if not using Google Colab. Check their official websites for details on how to install them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nv6UldrVuXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "import requests\n",
        "import random\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLX-9_KS2Xer",
        "colab_type": "text"
      },
      "source": [
        "To learn word embeddings, in this case Word2Vec (Mikolov et al. 2013 [link text](https://arxiv.org/pdf/1301.3781.pdf)), we first need a sufficiently large text corpus. To this end we are going to use the IMDb review corpus (the same used in Coursework 1), which includes all sentences.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR3p8oif2EC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url_train=\"http://josecamachocollados.com/imdb_train.txt\" # Containing all sentences of the imdb training set\n",
        "\n",
        "#Load training set\n",
        "response_train = requests.get(url_train)\n",
        "dataset_file_train = response_train.text.split(\"\\n\")\n",
        "random.shuffle(dataset_file_train) # We shuffle all sentences of our corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2QCPUMVjWoO",
        "colab_type": "text"
      },
      "source": [
        "Let's check what the IMDb dataset looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hDooBLLjZgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in dataset_file_train[:5]:\n",
        "  print (line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIyQsNSvd5wr",
        "colab_type": "text"
      },
      "source": [
        "Once the dataset is loaded, we are going to tokenize and store our corpus into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGig7uiMd-DV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gensim_imdb_corpus=[]\n",
        "for line in dataset_file_train:\n",
        "  gensim_imdb_corpus.append(word_tokenize(line))\n",
        "# This could take a lot of memory. If it's the case you can reduce the number of lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hey7GEQajBdf",
        "colab_type": "text"
      },
      "source": [
        "**Note:** The corpus may be further preprocessed if necessary (e.g. lowercased) or further cleaned. In this case the version of the IMDb corpus that we use was already lowercased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwJaJMhqPkQs",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can train our Word2Vec word embedding model! For more information on training Word2Vec with gensim, you can check [here](https://radimrehurek.com/gensim/models/word2vec.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kEGIWldgs7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A6xkP5-fhli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Word2Vec(gensim_imdb_corpus, size=100, window=5, min_count=3) \n",
        "# Size is the number of dimensions of the embeddings we are going to learn\n",
        "# Window is the size considered for context of a target word\n",
        "# Min count is the minimum number of times that a word need to occur to be learnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jutqmbRNYmlW",
        "colab_type": "text"
      },
      "source": [
        "**Note:** You can save and load models using `model.save` and `model.load` functions. This enables you to export your models and use them anytime, or to use models training by someone else (i.e. pre-trained models). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGwlOLAZEcB",
        "colab_type": "text"
      },
      "source": [
        "**Exercise (Optional):** Train the same model using [FastText](https://radimrehurek.com/gensim/models/fasttext.html) instead of Word2Vec. FastText is a model similar to Word2Vec but takes also into account character information, which can be useful for noisy text such as the one we find in social media."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfrdlU2Wg1m3",
        "colab_type": "text"
      },
      "source": [
        "## PLAYING WITH WORD2VEC\n",
        "\n",
        "---\n",
        "\n",
        "Now that our model has been trained, we can check the vectors for each word, which should have 100 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nijqa8REXHpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector_movie=model['movie']\n",
        "print (\"Number of dimensions: \"+str(len(vector_movie)))\n",
        "print (vector_movie)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqQ3uAbGXIDi",
        "colab_type": "text"
      },
      "source": [
        "We can also check the similarity (measured by cosine similarity) between some words. Let's start with finding the most similar words to *film* or *casablanca* in our vector space. We can find the most similar words of any input word by using the `.most_similar` command. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb2wSkXghXgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.most_similar('movie')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8jW-MkLkF20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.most_similar('casablanca')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snfhraN_jo3J",
        "colab_type": "text"
      },
      "source": [
        "We can also check the similarity between two given words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fox-nDRWkikv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.similarity('movie', 'film'))\n",
        "print(model.similarity('movie', 'popcorn'))\n",
        "print(model.similarity('movie', 'table'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2UKr5nclEW2",
        "colab_type": "text"
      },
      "source": [
        "Here we can see how words like *movie* and *film* are very close (in fact they are synonyms). Then other words like *movie* and *popcorn* are somehow related, while *movie* and *table* do not seem to be similar at all in this corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdsDDtiycQxs",
        "colab_type": "text"
      },
      "source": [
        "**Note:** In this notebook we have learned our own word embeddings in IMDb. However, please note that in many cases we are going to directly use an available pre-trained word embedding model. These are generally trained on large corpora and are therefore more complete/accurate. For example, there are pre-trained models for [Word2Vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/) or even [FastText trained on Twitter](https://github.com/pedrada88/crossembeddings-twitter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLSIIwhaeCA",
        "colab_type": "text"
      },
      "source": [
        "**Exercise (optional):** Choose a pre-trained model from Word2Vec, GloVe or FastText (there are many available online) and load it using gensim. Check a few similarities and compare it with the word embeddings trained on IMDb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meWk4w0bgFvm",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 1:** Train a Word2Vec word embedding model on the IMDb corpus with 75 dimensions and a window size of 8. Then, check the most similar words of *movie* in the vector space and the similarity between *movie* and *table*. Compare the results with the previous trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN0l2ynrgw3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To complete here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlBq1ifvZWUC",
        "colab_type": "text"
      },
      "source": [
        "**Exercise (optional):** Take a corpus of your choice (e.g. from one of the NLP projects) and train a word embedding model using gensim. Check a few similarities of words and compare with the models trained on IMDb."
      ]
    }
  ]
}