{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "4_training_linear_models.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn-5RznvUSvI",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 4 – Training Linear Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb25sF9NUSvP",
        "colab_type": "text"
      },
      "source": [
        "_This notebook code is taken/adapted from Aurélien Géron 2017_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4mDHUtBTPty",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nodTq9sFUSvR",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHU_cia1USvV",
        "colab_type": "text"
      },
      "source": [
        "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s87jJctlUSvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaRVL-IUSvh",
        "colab_type": "text"
      },
      "source": [
        "# Linear regression using the Normal Equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs7NhPK7USvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)    # uniformly generate 100 random numbers, i.e., 100 rows and 1 column\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)   # a linear function y = 4 + 3 * X, plus Gaussian noise N(0, 1) (normal distribution)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZQ7cFSEzq4z",
        "colab_type": "text"
      },
      "source": [
        "Plot the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY-a_NeXUSvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbFLgRr60Vjb",
        "colab_type": "text"
      },
      "source": [
        "Find model parameters using normal equation ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIIAAAAYCAYAAAA2/iXYAAAEiElEQVRoBe1YvS9sQRQ/u3kl1TZEFLbYUoJQCwrCv4CGVRLrD9D7iEIjlEKhZEWBCI2IjyjZmtUohEI3L79Jzs3Zu3PvnXvv7vLe3ZNM5uvM+Z6ZM5NSSilqQuItkE68BZoG0Bb487/b4evri66urqrUbGtro56enqrxRgw8PDxQe3s7QYawUC99/vsT4eLignZ2dujl5YUmJyd1jf75+XlYH8TGL5VKlM/nqbe3l25vbyPRq5c+gYHAwqdSKa0E+j8Bx8fHNDY2RthNYSCdTtPm5iZ9fHzoZX19fTQ/P09DQ0NhyGjctbW1yDYol8t0eXkZia8UtJb6SLqEZNEL9vf3VS6XU+vr66pcLivuo24krK6uqsXFRfX5+emwRXt4eBiJri6ZTEbd39/rgjbGMc9rpqamVDabVa+vrw4NNMLSeX5+VqOjo5pPBSHLzuHhoZYNtYSwcnjpI2mGaRsDAULNzs5qx7MhJVEEwtLSkmNkOVfrNgwmHSrps1HhdAQLAI6Gw/v7+xWcBmAjw3gmsKXDaxFwoM9BBT6wRz6fryrFYpGX6Zp5oXYDz8XVx03Xpm8MBJuFjcBhB5qMBv48D8Oxk1FLJwEPjsMpwcHilt2WjlwHPpJeqVRSpgLaEtjZJp1s5QjSR/KzbRsDAQrCuCgsMO803p0sjMSxZWqL5955pnVsWJwChULBePwzDusSlY5cB1qmq0bimNpBsvB8HH22t7d14MM3HKzsL75C3bIZA0FGJhtPCshHInYFB4YkLNdzQJlq01pJB0oE4UheJsfIeb/cRuKZ6Ei50IZhkT+htgU4CKcVbIEafTcEySHnTfpAHtDFHPhIXbz8BRmM/wgtLS3U0dGhk8qnpyeamJigg4ODiiQT79nv729aWVkh4EtA//T0VA7VrS1l7erqotbW1gpetrIE0akgGrEzMzNDKH4QJEeQPvgbQcErJZvN6tfS29ubtgue0AsLC1X+gjyez8fu7m5H3qOjIyoWi5TL5RzCW1tbNDAw8GOfMixcoVCg6+trymQydHZ2RnhnR4Fa0YnCW66plRzYENgY7+/v+u+E7TI4OCjZOW3PQGCMk5MT/e7GW7yzs1MP7+3t0c3NDc3NzTFaRY3TYmRkhPD34FeAA9yoMD09TY+Pj3R3d0fj4+OazMbGRmiataITVQ9eV0s5cHLg3wWwu7tLCDDT6c28jTkC7gzOCXDPcMKBO8Z977jvuFr2cd+5XwBM333fAReJkFcyxOvcdVQ6sI+8f910w/ajyuHHR/oQbT/wDAQ2LAREggLg10QQUT+GYeY4MZL8kKgigeSEixPX5eVlPcbj/IfgxS8uHdgFJS7ElcOPv/ShHx7mPAMhaGGj5hEEQS+HRsnCfPxOKsb5DbXNq4vl/PWBAEGhkPuLmRVodB33i7ne8uKUwubB8xFf4XxiBvH9JwIBSuCrNs4ff5AhbOYRkPh6D7p2bGjVA4evUlyPkJOvdBteKSA5mWOzkVgLBD4fE2uZhCneDISEOdxL3WYgeFkmYePNQEiYw73UbQaCl2USNv4XOibNjTdXUqEAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWG-pqVrUSv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance, this introduces a constant feature to incorperate b=w0 into parameter vector\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)    #  theta_best is w in above normal equation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11x4vMN4USwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta_best  # print model parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FlXBhPX4hT_",
        "colab_type": "text"
      },
      "source": [
        "Using the learning model to predict y values for two given values of x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6AQOZafUSwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_new = np.array([[0], [2]])             # two x values\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add constant feature x0 = 1 to each instance\n",
        "y_predict = X_new_b.dot(theta_best)      # make prediction using the learning model y = Xw\n",
        "y_predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm2wx0ia5Gv4",
        "colab_type": "text"
      },
      "source": [
        "Plot the model (red line) along with the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y7i8V31USwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCeqxu986NwN",
        "colab_type": "text"
      },
      "source": [
        "You can also use LinearRegression of sklearn to learn the model as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_gAIRslUSwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)                   # note that it takes X instead of X_b, as it treats the intercept b as a separate parameter\n",
        "lin_reg.intercept_, lin_reg.coef_   # show model parameters. Note intercept_ is w0, coef_ are other parameters, i.e., w1 in this example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOHBsH9QUSw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg.predict(X_new)  # predict for the given x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKoTvjArUSw9",
        "colab_type": "text"
      },
      "source": [
        "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\"), which you could call directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKNeB-oDUSw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
        "theta_best_svd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iS_bID5USxH",
        "colab_type": "text"
      },
      "source": [
        "This function computes $\\mathbf{X}^+\\mathbf{y}$, where $\\mathbf{X}^{+}$ is the _pseudoinverse_ of $\\mathbf{X}$ (specifically the Moore-Penrose inverse). You can use `np.linalg.pinv()` to compute the pseudoinverse directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFvO3z2JUSxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.linalg.pinv(X_b).dot(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QG4sh0KUSxT",
        "colab_type": "text"
      },
      "source": [
        "**Note**: the first releases of the book implied that the `LinearRegression` class was based on the Normal Equation. This was an error, my apologies: as explained above, it is based on the pseudoinverse, which ultimately relies on the SVD matrix decomposition of $\\mathbf{X}$ (see chapter 8 for details about the SVD decomposition). Its time complexity is $O(n^2)$ and it works even when $m < n$ or when some features are linear combinations of other features (in these cases, $\\mathbf{X}^T \\mathbf{X}$ is not invertible so the Normal Equation fails), see [issue #184](https://github.com/ageron/handson-ml/issues/184) for more details. However, this does not change the rest of the description of the `LinearRegression` class, in particular, it is based on an analytical solution, it does not scale well with the number of features, it scales linearly with the number of instances, all the data must fit in memory, it does not require feature scaling and the order of the instances in the training set does not matter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPtY6CRj9LeN",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "Generate 100 examples using a linear function with 3 features/variables, add a Gaussian noise *N*(0,1) to it. For example, \n",
        "y = 4 + 2*x_1 + 3* x_2 + 4 x_3 + N(0,1). Train a linear model to fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I89MBPWjUSxW",
        "colab_type": "text"
      },
      "source": [
        "# Linear regression using batch gradient descent\n",
        "BGD calculates gradient over all data points ![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j460_KdUSxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eta = 0.1   # learning rate\n",
        "n_iterations = 1000   # number of interation\n",
        "m = 100   # number of data points\n",
        "theta = np.random.randn(2,1)  # parameter initialisation\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) \n",
        "    theta = theta - eta * gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ5tedpdUSxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVX_KZakUSxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_new_b.dot(theta)    # prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTh7UXkgTAbv",
        "colab_type": "text"
      },
      "source": [
        "Define a function to show intermediate models of a training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUkSsBojUSx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta_path_bgd = []\n",
        "\n",
        "def plot_gradient_descent(theta, eta, theta_path=None):\n",
        "    m = len(X_b)\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    n_iterations = 1000\n",
        "    for iteration in range(n_iterations):\n",
        "        if iteration < 10:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if iteration > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "        theta = theta - eta * gradients\n",
        "        if theta_path is not None:\n",
        "            theta_path.append(theta)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 2, 0, 15])\n",
        "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpZvVZCPTV_4",
        "colab_type": "text"
      },
      "source": [
        "Show effect of different learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGJInbLUSx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
        "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1H9KOFKUSyF",
        "colab_type": "text"
      },
      "source": [
        "# Stochastic Gradient Descent\n",
        "SGD calculates gradient of the loss function and then updates the model parameters using only a single point which is randomly selected from the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV68HhK4USyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta_path_sgd = []\n",
        "m = len(X_b)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCzvmrUif3sX",
        "colab_type": "text"
      },
      "source": [
        "Implement SGD from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2SA6AiAUSye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # learning schedule hyperparameters, this gives initial learning rate 0.1\n",
        "\n",
        "def learning_schedule(t):     # function to derease learning rate over training steps\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization of w0 and w1\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):        # m is the size of the training set\n",
        "        if epoch == 0 and i < 20:                    # not shown in the book\n",
        "            y_predict = X_new_b.dot(theta)           # not shown\n",
        "            style = \"b-\" if i > 0 else \"r--\"         # not shown\n",
        "            plt.plot(X_new, y_predict, style)        # not shown\n",
        "        random_index = np.random.randint(m)          # selection a random point from the trainig set\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi) # calculate gradient based on current single point\n",
        "        eta = learning_schedule(epoch * m + i)       # gradually decrease learning rate each step\n",
        "        theta = theta - eta * gradients              # update model parameters based current single point\n",
        "        theta_path_sgd.append(theta)                 # not shown\n",
        "\n",
        "plt.plot(X, y, \"b.\")                                 # not shown\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)                     # not shown\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           # not shown\n",
        "plt.axis([0, 2, 0, 15])                              # not shown\n",
        "plt.show()                                           # not shown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THVrZWrYUSyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md-swZTegHMk",
        "colab_type": "text"
      },
      "source": [
        "Implement SGD using SGDRegressor of sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf7byjg_USyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSXN1LzpUSy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd_reg.intercept_, sgd_reg.coef_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok-1nm0hUSy9",
        "colab_type": "text"
      },
      "source": [
        "# Mini-batch gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnUXiHtDUSzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta_path_mgd = []\n",
        "\n",
        "n_iterations = 50\n",
        "minibatch_size = np.int(m/5)\n",
        "\n",
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "t0, t1 = 200, 1000            # this gives a initial learning rate of 0.2\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(n_iterations):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, minibatch_size):\n",
        "        t += 1\n",
        "        xi = X_b_shuffled[i:i+minibatch_size]\n",
        "        yi = y_shuffled[i:i+minibatch_size]\n",
        "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(t)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_mgd.append(theta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_2Dw021USzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCPtNKt_USzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta_path_bgd = np.array(theta_path_bgd)\n",
        "theta_path_sgd = np.array(theta_path_sgd)\n",
        "theta_path_mgd = np.array(theta_path_mgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhhcR80aUSza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
        "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
        "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
        "plt.legend(loc=\"upper left\", fontsize=16)\n",
        "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
        "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
        "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FETfMfUQUSzi",
        "colab_type": "text"
      },
      "source": [
        "# Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkHQXts0USzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_nF74JAUSzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3                # generate 100 values between -3 and 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)  # a quadratic function plus Gaussian noise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGVfpOy2USz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjcns-XYUSz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)  # create a 2nd degree feature\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "X[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-GyQ1LUS0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_poly[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL3TcVOaUS0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.intercept_, lin_reg.coef_   # print the learned model parameters. note the original function y = 0.5 * X**2 + X + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrZSmmTD6c6Z",
        "colab_type": "text"
      },
      "source": [
        "Plot the learned prediction model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af6o_PqIUS0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHdhLmQO7GAE",
        "colab_type": "text"
      },
      "source": [
        "Illustration of overfitting and underfitting through different polynomial degrees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzXVo7-VUS0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
        "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    std_scaler = StandardScaler()\n",
        "    lin_reg = LinearRegression()\n",
        "    polynomial_regression = Pipeline([\n",
        "            (\"poly_features\", polybig_features),\n",
        "            (\"std_scaler\", std_scaler),\n",
        "            (\"lin_reg\", lin_reg),\n",
        "        ])\n",
        "    polynomial_regression.fit(X, y)\n",
        "    y_newbig = polynomial_regression.predict(X_new)\n",
        "    plt.plot(X_new, y_newbig, style, label='degree '+str(degree), linewidth=width)\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB3XAR9I80FE",
        "colab_type": "text"
      },
      "source": [
        "Plot learning curve MSE with respect to training set size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfEzE8oPUS0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    # split dataset for training and test, 20% for test\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    train_errors, val_errors = [], []\n",
        "    for m in range(1, len(X_train)):    # vary training set size\n",
        "        model.fit(X_train[:m], y_train[:m])           # fit the model\n",
        "        y_train_predict = model.predict(X_train[:m])  # prediction of training data\n",
        "        y_val_predict = model.predict(X_val)          # prediction of test data\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))   # MSE on training set\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))             # MSE on test set\n",
        "\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
        "    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
        "    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
        "    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_aQ2LG9_MV1",
        "colab_type": "text"
      },
      "source": [
        "Linear regression results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4bI1cZpUS0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg = LinearRegression()\n",
        "plot_learning_curves(lin_reg, X, y)\n",
        "plt.axis([0, 80, 0, 3])                         # not shown in the book\n",
        "\n",
        "plt.show()                                      # not shown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtIHfIfa_T3U",
        "colab_type": "text"
      },
      "source": [
        "Polynomial regression results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmqxdKzBUS0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 3])           # not shown\n",
        "\n",
        "plt.show()                        # not shown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suleXh6uUS0w",
        "colab_type": "text"
      },
      "source": [
        "# Regularized models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPqut6MPE0rJ",
        "colab_type": "text"
      },
      "source": [
        "Demonstration of **ridge regression** with and without polynomial terms, and with different regularization strength"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFcgqUKGUS0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
        "\n",
        "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
        "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
        "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
        "        if polynomial:\n",
        "            model = Pipeline([\n",
        "                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
        "                    (\"std_scaler\", StandardScaler()),\n",
        "                    (\"regul_reg\", model),\n",
        "                ])\n",
        "        model.fit(X, y)\n",
        "        y_new_regul = model.predict(X_new)\n",
        "        lw = 2 if alpha > 0 else 1\n",
        "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
        "    plt.plot(X, y, \"b.\", linewidth=3)\n",
        "    plt.legend(loc=\"upper left\", fontsize=15)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 3, 0, 4])\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9JsBtYUHX9d",
        "colab_type": "text"
      },
      "source": [
        "Ridge regression using \"cholesky\" solver, which is the standard scipy.linalg.solve function to obtain a closed-form solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7L1nZjlUS05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTUKWWzDUS1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=\"l2\", random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do_qcftmIgau",
        "colab_type": "text"
      },
      "source": [
        "Uses a Stochastic Average Gradient descent \"sag\" solver."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuXgy4vZUS1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQqZ7LLGJEED",
        "colab_type": "text"
      },
      "source": [
        "Demonstration of **lasso** with and without polynomial terms, and with different regularization strength"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrz773JnUS1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), tol=1, random_state=42)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2JcfdiXUS1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "lasso_reg.predict([[1.5]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqB4VjLIKJqa",
        "colab_type": "text"
      },
      "source": [
        "**Elastic Net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCrAIZ3tUS1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(X, y)\n",
        "elastic_net.predict([[1.5]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbd8RQJVK_lX",
        "colab_type": "text"
      },
      "source": [
        "Demonstration of **early stopping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7QZewKAzUS1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)\n",
        "\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler()),\n",
        "    ])\n",
        "\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1,\n",
        "                       tol=-np.infty,\n",
        "                       penalty=None,\n",
        "                       eta0=0.0005,\n",
        "                       warm_start=True,\n",
        "                       learning_rate=\"constant\",\n",
        "                       random_state=42)\n",
        "\n",
        "n_epochs = 500\n",
        "train_errors, val_errors = [], []\n",
        "for epoch in range(n_epochs):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
        "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
        "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "best_epoch = np.argmin(val_errors)\n",
        "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
        "\n",
        "plt.annotate('Best model',\n",
        "             xy=(best_epoch, best_val_rmse),\n",
        "             xytext=(best_epoch, best_val_rmse + 1),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             fontsize=16,\n",
        "            )\n",
        "\n",
        "best_val_rmse -= 0.03  # just to make the graph look better\n",
        "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
        "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
        "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
        "plt.legend(loc=\"upper right\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"RMSE\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QeteKuQM5-p",
        "colab_type": "text"
      },
      "source": [
        "Code to save the best model as training progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrMGZoWhUS15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import clone\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None,\n",
        "                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error:\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = clone(sgd_reg)   # store the best model in best_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qad7dw2rUS1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_epoch, best_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm31E0j4PFwM",
        "colab_type": "text"
      },
      "source": [
        "Illustration of parameter trajectory of Lasso versus Ridge regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx6CKn9GUS2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n",
        "\n",
        "# ignoring bias term\n",
        "t1s = np.linspace(t1a, t1b, 500)\n",
        "t2s = np.linspace(t2a, t2b, 500)\n",
        "t1, t2 = np.meshgrid(t1s, t2s)\n",
        "T = np.c_[t1.ravel(), t2.ravel()]\n",
        "Xr = np.array([[-1, 1], [-0.3, -1], [1, 0.1]])\n",
        "yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n",
        "\n",
        "J = (1/len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis=1)).reshape(t1.shape)\n",
        "\n",
        "N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\n",
        "N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n",
        "\n",
        "t_min_idx = np.unravel_index(np.argmin(J), J.shape)\n",
        "t1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n",
        "\n",
        "t_init = np.array([[0.25], [-1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u30gJsrUS2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bgd_path(theta, X, y, l1, l2, core = 1, eta = 0.1, n_iterations = 50):\n",
        "    path = [theta]\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = core * 2/len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + 2 * l2 * theta\n",
        "\n",
        "        theta = theta - eta * gradients\n",
        "        path.append(theta)\n",
        "    return np.array(path)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, N, l1, l2, title in ((0, N1, 0.5, 0, \"Lasso\"), (1, N2, 0,  0.1, \"Ridge\")):\n",
        "    JR = J + l1 * N1 + l2 * N2**2\n",
        "    \n",
        "    tr_min_idx = np.unravel_index(np.argmin(JR), JR.shape)\n",
        "    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n",
        "\n",
        "    levelsJ=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)\n",
        "    levelsJR=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)\n",
        "    levelsN=np.linspace(0, np.max(N), 10)\n",
        "    \n",
        "    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n",
        "    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n",
        "    path_N = bgd_path(t_init, Xr, yr, np.sign(l1)/3, np.sign(l2), core=0)\n",
        "\n",
        "    plt.subplot(221 + i * 2)\n",
        "    plt.grid(True)\n",
        "    plt.axhline(y=0, color='k')\n",
        "    plt.axvline(x=0, color='k')\n",
        "    plt.contourf(t1, t2, J, levels=levelsJ, alpha=0.9)\n",
        "    plt.contour(t1, t2, N, levels=levelsN)\n",
        "    plt.plot(path_J[:, 0], path_J[:, 1], \"w-o\")\n",
        "    plt.plot(path_N[:, 0], path_N[:, 1], \"y-^\")\n",
        "    plt.plot(t1_min, t2_min, \"rs\")\n",
        "    plt.title(r\"$\\ell_{}$ penalty\".format(i + 1), fontsize=16)\n",
        "    plt.axis([t1a, t1b, t2a, t2b])\n",
        "    if i == 1:\n",
        "        plt.xlabel(r\"$\\theta_1$\", fontsize=20)\n",
        "    plt.ylabel(r\"$\\theta_2$\", fontsize=20, rotation=0)\n",
        "\n",
        "    plt.subplot(222 + i * 2)\n",
        "    plt.grid(True)\n",
        "    plt.axhline(y=0, color='k')\n",
        "    plt.axvline(x=0, color='k')\n",
        "    plt.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n",
        "    plt.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n",
        "    plt.plot(t1r_min, t2r_min, \"rs\")\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.axis([t1a, t1b, t2a, t2b])\n",
        "    if i == 1:\n",
        "        plt.xlabel(r\"$\\theta_1$\", fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhnlV-gwUS2R",
        "colab_type": "text"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXXwtQePilf",
        "colab_type": "text"
      },
      "source": [
        "Plot **logistic function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjx6HeqlUS2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.linspace(-10, 10, 100)\n",
        "sig = 1 / (1 + np.exp(-t))\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.plot([-10, 10], [0, 0], \"k-\")\n",
        "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
        "plt.plot([-10, 10], [1, 1], \"k:\")\n",
        "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
        "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.legend(loc=\"upper left\", fontsize=20)\n",
        "plt.axis([-10, 10, -0.1, 1.1])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EakhsFsnPgaF",
        "colab_type": "text"
      },
      "source": [
        "Demontration of classification of Iris data using **logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bu3HiagUS2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6GDZGOUS2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(iris.DESCR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RttKABtpUS2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris[\"data\"][:, 3:]  # petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WzoqhKnUS2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
        "log_reg.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8qkWxAbUS2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nu1Uo5FUS21",
        "colab_type": "text"
      },
      "source": [
        "The figure in the book actually is actually a bit fancier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT1RgHMJUS22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(X[y==0], y[y==0], \"bs\")\n",
        "plt.plot(X[y==1], y[y==1], \"g^\")\n",
        "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")\n",
        "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
        "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
        "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
        "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
        "plt.ylabel(\"Probability\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 3, -0.02, 1.02])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knN6NliTUS2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decision_boundary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXwcfX4aUS3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_reg.predict([[1.7], [1.5]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVREM4FyUS3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)\n",
        "\n",
        "log_reg = LogisticRegression(solver=\"liblinear\", C=10**10, random_state=42)\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
        "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
        "\n",
        "zz = y_proba[:, 1].reshape(x0.shape)\n",
        "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
        "\n",
        "\n",
        "left_right = np.array([2.9, 7])\n",
        "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
        "\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
        "plt.text(3.5, 1.5, \"Not Iris-Virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
        "plt.text(6.5, 2.3, \"Iris-Virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.axis([2.9, 7, 0.8, 2.7])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUXgQwjvUS3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
        "softmax_reg.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMo9M2XWUS3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "\n",
        "y_proba = softmax_reg.predict_proba(X_new)\n",
        "y_predict = softmax_reg.predict(X_new)\n",
        "\n",
        "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris-Virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris-Versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris-Setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5rG6a5YUS3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_reg.predict([[5, 2]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9baQktGSUS3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_reg.predict_proba([[5, 2]])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}